{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAG: Document Retrieval and Generation**\n",
    "\n",
    "The goal of this notebook is to demonstrate the process of document retrieval and generation using a Retrieval-Augmented Generation (RAG) approach. It showcases how to index a set of documents using embeddings and FAISS, retrieve the most relevant documents based on a given query, and then generate a contextually accurate response by concatenating the retrieved documents with a prompt.\n",
    "\n",
    "**1. Library Testing and GPU Availability**:\n",
    "\n",
    "In this section, it will be tested and checked the availability of several libraries including:\n",
    "\n",
    "- **scikit-learn** (sklearn)\n",
    "- **FAISS** (Facebook AI Similarity Search)\n",
    "- **Transformers** (Hugging Face)\n",
    "- **TensorFlow**\n",
    "- **PyTorch** (torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-gpu-cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "print(faiss.get_num_gpus())  # Should print the number of GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers library is working!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "print(\"Transformers library is working!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Built with CUDA: True\n",
      "Built with cuDNN: True\n",
      "GPU Device Name: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-26 13:59:09.249212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.249910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.249993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.251136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.251241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.251301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.316941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.317231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.317444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-26 13:59:09.317637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /device:GPU:0 with 4220 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Is GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Built with cuDNN:\", tf.test.is_built_with_gpu_support())\n",
    "print(\"GPU Device Name:\", tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "cuDNN Version: 90100\n",
      "Is cuDNN Enabled: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"cuDNN Version:\", torch.backends.cudnn.version())\n",
    "print(\"Is cuDNN Enabled:\", torch.backends.cudnn.enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Retrieval-augmented generation (RAG)**\n",
    "\n",
    "**2. Example Execution:**\n",
    "\n",
    "A simple retrieval-augmented generation (RAG) pipeline is created using the Hugging Face Transformers library. A pre-trained model and tokenizer are loaded, followed by a retrieval task using FAISS, and text is generated based on the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace token\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # load variables from .env \n",
    "\n",
    "hf_token=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Torre Eiffel tem 324 metros de altura. Qual é a altura da Torre Eiffel?\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel\n",
    "import faiss\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Data example\n",
    "documents = [\n",
    "    \"A Torre Eiffel tem 324 metros de altura.\",\n",
    "    \"A Estátua da Liberdade fica em Nova York.\",\n",
    "    \"O Monte Everest é a montanha mais alta do mundo.\"\n",
    "]\n",
    "\n",
    "# Document Vectorization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def embed(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings.numpy()\n",
    "\n",
    "document_embeddings = embed(documents)   \n",
    "\n",
    "# Indexing with FAISS\n",
    "index = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# Generator Model\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/ptt5-base-portuguese-vocab\") # available for pt-br language\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/ptt5-base-portuguese-vocab\")\n",
    "\n",
    "# Search and Generation Function\n",
    "def rag(question, top_k=1):\n",
    "    # Question Embedding\n",
    "    question_embedding = embed([question])\n",
    "    # Retrieval of the Most Similar Documents\n",
    "    distances, indices = index.search(question_embedding, top_k)\n",
    "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
    "    # Concatenate Retrieved Documents to the Prompt\n",
    "    input_text = \" \".join(retrieved_docs) + \" \" + question\n",
    "    inputs = generator_tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    with warnings.catch_warnings():\n",
    "       warnings.simplefilter(\"ignore\")\n",
    "    outputs = generator_model.generate(inputs, max_length=50, num_beams=2)\n",
    "    answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer \n",
    "\n",
    "# Usage Example\n",
    "question = \"Qual é a altura da Torre Eiffel?\"\n",
    "print(rag(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.1428781896829605, 'token': 5028, 'token_str': 'pedra', 'sequence': 'Tinha uma pedra no meio do caminho.'}, {'score': 0.062133897095918655, 'token': 7411, 'token_str': 'árvore', 'sequence': 'Tinha uma árvore no meio do caminho.'}, {'score': 0.05514989048242569, 'token': 5675, 'token_str': 'estrada', 'sequence': 'Tinha uma estrada no meio do caminho.'}, {'score': 0.029918920248746872, 'token': 1105, 'token_str': 'casa', 'sequence': 'Tinha uma casa no meio do caminho.'}, {'score': 0.025660440325737, 'token': 3466, 'token_str': 'cruz', 'sequence': 'Tinha uma cruz no meio do caminho.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForMaskedLM.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
    "\n",
    "# Create the pipeline for the fill-mask task\n",
    "pipe = pipeline('fill-mask', model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Test the pipeline\n",
    "result = pipe('Tinha uma [MASK] no meio do caminho.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Fale sobre energia renovável\n",
      "Resposta: summarize: question: Fale sobre energia renovável context: A energia renovável ajuda a reduzir a emissão de gases de efeito estufa.Investir em energia renovável pode impulsionar a economia verde. A energia renovável é obtida de recursos naturais que se regeneram naturalmente.\n"
     ]
    }
   ],
   "source": [
    "# Another Example - RAG\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sentence_transformers import SentenceTransformer \n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Simulating a Document Database\n",
    "documents = [\n",
    "    \"A energia renovável é obtida de recursos naturais que se regeneram naturalmente.\",\n",
    "    \"Os principais tipo de energia renovável são solar, eólica, hidroelétrica e biomassa.\",\n",
    "    \"A energia renovável ajuda a reduzir a emissão de gases de efeito estufa.\"\n",
    "    \"Investir em energia renovável pode impulsionar a economia verde.\"\n",
    "]\n",
    "\n",
    "# Step 1: Document Indexing Using Embeddings\"\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "document_embeddings = embedder.encode(documents)\n",
    "\n",
    "# Creating the FAISS Index\"\n",
    "d = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(document_embeddings)\n",
    "\n",
    "# Function to Retrieve Relevant Documents\n",
    "def retrieve_documents(query, k=2):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
    "    return retrieved_docs\n",
    "\n",
    "# Load T5 tokenizer and model fine-tuned on Portuguese\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"unicamp-dl/ptt5-base-portuguese-vocab\")\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\"unicamp-dl/ptt5-base-portuguese-vocab\")\n",
    "\n",
    "# Function to Generate the Response\n",
    "def generate_answer(question, retrieved_docs):\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    input_text = f\"summarize: question: {question} context: {context}\"\n",
    "    inputs = generator_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = generator_model.generate(**inputs, max_length=150, num_beams=5, early_stopping=True)\n",
    "    answer = generator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Usage Example\n",
    "question = \"Fale sobre energia renovável\"\n",
    "retrieved_docs = retrieve_documents(question)\n",
    "answer = generate_answer(question, retrieved_docs)\n",
    "print(\"Pergunta:\", question)\n",
    "print(\"Resposta:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "updated-langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
