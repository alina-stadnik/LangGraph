{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Groq**\n",
    "\n",
    "Groq is a natural language processing framework that enables structured interaction between large language models (LLMs) and external tools or APIs. It provides a standardized interface to define and manage tool callsâ€”functions or services invoked by the LLM to handle specialized tasks.\n",
    "\n",
    "### **Introduction to Tool Use**\n",
    "\n",
    "Tool use is a powerful feature that allows Large Language Models (LLMs) to interact with external resources, such as APIs, databases, and the web, to gather dynamic data they wouldn't otherwise have access to in their pre-trained (or static) state and perform actions beyond simple text generation.\n",
    "\n",
    "Tool use bridges the gap between the data that the LLMs were trained on with dynamic data and real-world actions, which opens up a wide array of realtime use cases for us to build applications with.\n",
    "\n",
    "#### **How Tool Use Works:**\n",
    "\n",
    "To integrate tools with Groq API, follow these steps:\n",
    "\n",
    "1. Provide tools (or predefined functions) to the LLM for performing actions and accessing external data in real-time in addition to your user prompt within your Groq API request.\n",
    "2. Define how the tools should be used to teach the LLM how to use them effectively (e.g. by defining input and output formats).\n",
    "3. Let the LLM autonomously decide whether or not the provided tools are needed for a user query by evaluating the user query, determining whether the tools can enhance its response, and utilizing the tools accordingly.\n",
    "4. Extract tool input, execute the tool code, and return results.\n",
    "5. Let the LLM use the tool result to formulate a response to the original prompt.\n",
    "\n",
    "This process allows the LLM to perform tasks such as real-time data retrieval, complex calculations, and external API interaction, all while maintaining a natural conversation with our end user.\n",
    "\n",
    "#### **Tool Use with Groq**\n",
    "\n",
    "Groq API endpoints support tool use to almost instantly deliver structured JSON output that can be used to directly invoke functions from desired external resources.\n",
    "\n",
    "- **Supported Models:** \n",
    "\n",
    "Llama 3.3 and 3.1 Models\n",
    "\n",
    "The following Llama-3.3 models are recommended for tool use due to their versatility and performance:\n",
    "\n",
    "- llama-3.3-70b-versatile\n",
    "- llama-3.1-8b-instant\n",
    "\n",
    "\n",
    "- **Other Supported Models:**\n",
    "\n",
    "The following models powered by Groq also support tool use:\n",
    "\n",
    "- llama3-70b-8192\n",
    "- llama3-8b-8192\n",
    "- mixtral-8x7b-32768 (parallel tool use not supported)\n",
    "- gemma2-9b-it (parallel tool use not supported)\n",
    "\n",
    "#### **Tools Specifications**\n",
    "\n",
    "Tool Call and Tool Response Structure: \n",
    "\n",
    "**Tool Call Structure:**\n",
    "\n",
    "```\n",
    "{\n",
    "  \"model\": \"llama-3.3-70b-versatile\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a weather assistant. Use the get_weather function to retrieve weather information for a given location.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What's the weather like in New York today?\"\n",
    "    }\n",
    "  ],\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get the current weather for a location\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"location\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "            },\n",
    "            \"unit\": {\n",
    "              \"type\": \"string\",\n",
    "              \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "              \"description\": \"The unit of temperature to use. Defaults to fahrenheit.\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"location\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"tool_choice\": \"auto\",\n",
    "  \"max_tokens\": 4096\n",
    "}'\n",
    "```\n",
    "\n",
    "**Tool Call Response:**\n",
    "\n",
    "```\n",
    "\"model\": \"llama-3.3-70b-versatile\",\n",
    "\"choices\": [{\n",
    "    \"index\": 0,\n",
    "    \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"tool_calls\": [{\n",
    "            \"id\": \"call_d5wg\",\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"arguments\": \"{\\\"location\\\": \\\"New York, NY\\\"}\"\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    \"logprobs\": null,\n",
    "    \"finish_reason\": \"tool_calls\"\n",
    "}],\n",
    "```\n",
    "\n",
    "When a model decides to use a tool, it returns a response with a tool_calls object containing:\n",
    "\n",
    "- id: a unique identifier for the tool call\n",
    "- type: the type of tool call, i.e. function\n",
    "- name: the name of the tool being used\n",
    "- parameters: an object containing the input being passed to the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting Up Tools with Groq**\n",
    "\n",
    "Note: In this example, we're defining a function as our tool, but your tool can be any function or an external resource (e.g. dabatase, web search engine, external API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_message ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_m8z6', function=Function(arguments='{\"expression\": \"25 * 4 + 10\"}', name='calculate'), type='function')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of 25 * 4 + 10 is 110.\n"
     ]
    }
   ],
   "source": [
    "# Groq Tool Calling\n",
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize the Groq client\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# Specify the model to be used \n",
    "MODEL = 'llama-3.3-70b-versatile'\n",
    "\n",
    "# Step 1: Create tool\n",
    "def calculate(expression):\n",
    "    \"\"\"\n",
    "    Evaluate a mathematical expression\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to evaluate the math expression\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "\n",
    "    except:\n",
    "        # Return an error message if the math expression is invalid\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})\n",
    "\n",
    "# Step 2: Pass Tool Definition and Messages to Model\n",
    "# Step 3: Receive and Handle Tool Results    \n",
    "def run_conversation(user_prompt):\n",
    "    # Initialize the conversation with system and user messages\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "    # Define the available tools (i.e. functions) for our model to use\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    # Make the initial API call to Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_tokens=4096 # Maximum number of tokens to allow in our response\n",
    "    )\n",
    "    # Extract the response and any tool call responses\n",
    "    response_message = response.choices[0].message\n",
    "    print('response_message', response_message)\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        # Define the available tools that can be called by the LLM\n",
    "        available_functions = {\n",
    "            \"calculate\": calculate,\n",
    "        }\n",
    "        # Add the LLM's response to the conversation\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            # Call the tool and get the response\n",
    "            function_response = function_to_call(\n",
    "                expression=function_args.get(\"expression\")\n",
    "            )\n",
    "            # Add the tool response to the conversation\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"role\": \"tool\", # Indicates this message is from tool use\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        # Make a second API call with the updated conversation\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Return the final response\n",
    "        return second_response.choices[0].message.content\n",
    "# Example usage\n",
    "user_prompt = \"What is 25 * 4 + 10?\"\n",
    "print(run_conversation(user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Routing System**\n",
    "\n",
    "It's possible to use different available models as part of a routing system:\n",
    "\n",
    "1. **Query Analysis**: Implement a routing system that analyzes incoming user queries to determine their nature and requirements.\n",
    "2. **Model Selection**: Based on the query analysis, route the request to the most appropriate model:\n",
    "    - For queries involving function calling, API interactions, or structured data manipulation, use the Llama 3 Groq Tool Use models.\n",
    "    - For general knowledge, open-ended conversations, or tasks not specifically related to tool use, route to a general-purpose language model, such as Llama 3 70B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of the Netherlands?\n",
      "Route: no tool needed\n",
      "Response: The capital of the Netherlands is Amsterdam!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Calculate 25 * 4 + 10\n",
      "Route: calculate tool needed\n",
      "Response: To calculate this, I'll follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Multiply 25 and 4: 25 * 4 = 100\n",
      "2. Add 10 to the result: 100 + 10 = 110\n",
      "\n",
      "So the final answer is 110!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "ROUTING_MODEL = \"llama3-70b-8192\"\n",
    "TOOL_USE_MODEL = \"llama-3.3-70b-versatile\"\n",
    "GENERAL_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"\n",
    "    Tool to evaluate a mathematical expression\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})\n",
    "\n",
    "def route_query(query):\n",
    "    \"\"\"Routing logic to let LLM decide if tools are needed\"\"\"\n",
    "    routing_prompt = f\"\"\"\n",
    "    Given the following user query, determine if any tools are needed to answer it.\n",
    "    If a calculation tool is needed, respond with 'TOOL: CALCULATE'.\n",
    "    If no tools are needed, respond with 'NO TOOL'.\n",
    "\n",
    "    User query: {query}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=ROUTING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a routing assistant. Determine if tools are needed based on the user query.\"},\n",
    "            {\"role\": \"user\", \"content\": routing_prompt}\n",
    "        ],\n",
    "        max_tokens=20  # We only need a short response\n",
    "    )\n",
    "    \n",
    "    routing_decision = response.choices[0].message.content.strip()\n",
    "    \n",
    "    if \"TOOL: CALCULATE\" in routing_decision:\n",
    "        return \"calculate tool needed\"\n",
    "    else:\n",
    "        return \"no tool needed\"\n",
    "\n",
    "def run_with_tool(query):\n",
    "    \"\"\"\n",
    "    Use the tool use model to perform the calculation\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",\n",
    "                \"description\": \"Evaluate a mathematical expression\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=TOOL_USE_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        messages.append(response_message)\n",
    "        for tool_call in tool_calls:\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = calculate(function_args.get(\"expression\"))\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": \"calculate\",\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=TOOL_USE_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        return second_response.choices[0].message.content\n",
    "    return response_message.content\n",
    "\n",
    "def run_general(query):\n",
    "    \"\"\"\n",
    "    Use the general model to answer the query since no tool is needed\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=GENERAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_query(query):\n",
    "    \"\"\"\n",
    "    Process the query and route it to the appropriate model\n",
    "    \"\"\"\n",
    "    route = route_query(query)\n",
    "    if route == \"calculate\":\n",
    "        response = run_with_tool(query)\n",
    "    else:\n",
    "        response = run_general(query)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"route\": route,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    \"What is the capital of the Netherlands?\",\n",
    "    \"Calculate 25 * 4 + 10\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = process_query(query)\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Route: {result['route']}\")\n",
    "    print(f\"Response: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DuckDuckGo**\n",
    "\n",
    "DuckDuckGo is a privacy-focused search engine designed to provide search results without tracking users' personal data. It emphasizes anonymity and delivers results through a combination of its own indexing and external sources like Bing and Wikipedia.\n",
    "\n",
    "- The DuckDuckGo Search (DDGS) API allows developers to fetch search results programmatically.\n",
    "- Supports text search, instant answers, and up to five results per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:primp:response: https://lite.duckduckgo.com/lite/ 200 15746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Curitiba, ParanÃ¡, Brazil Weather Forecast | AccuWeather', 'href': 'https://www.accuweather.com/en/br/curitiba/44944/weather-forecast/44944', 'body': 'Curitiba, ParanÃ¡, Brazil Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.'}, {'title': 'Weather in Curitiba, ParanÃ¡, Brazil - timeanddate.com', 'href': 'https://www.timeanddate.com/weather/brazil/curitiba', 'body': 'Current weather in Curitiba and forecast for today, tomorrow, and next 14 days'}, {'title': 'Curitiba, Brazil Weather Conditions | Weather Underground', 'href': 'https://www.wunderground.com/weather/br/curitiba', 'body': 'Curitiba Weather Forecasts. Weather Underground provides local & long-range weather forecasts, weatherreports, maps & tropical weather conditions for the Curitiba area.'}, {'title': 'Curitiba, ParanÃ¡, Brazil Current Weather | AccuWeather', 'href': 'https://www.accuweather.com/en/br/curitiba/44944/current-weather/44944', 'body': 'Current weather in Curitiba, ParanÃ¡, Brazil. Check current conditions in Curitiba, ParanÃ¡, Brazil with radar, hourly, and more.'}, {'title': 'Weather Today for Curitiba, ParanÃ¡, Brazil | AccuWeather', 'href': 'https://www.accuweather.com/en/br/curitiba/44944/weather-today/44944', 'body': \"Everything you need to know about today's weather in Curitiba, ParanÃ¡, Brazil. High/Low, Precipitation Chances, Sunrise/Sunset, and today's Temperature History.\"}]\n"
     ]
    }
   ],
   "source": [
    "# Using duckduckgo \n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "results = DDGS().text('Curitiba temperature', max_results=5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Response Message: ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_9ggc', function=Function(arguments='{\"question\": \"PrevisÃ£o do tempo em Curitiba, ParanÃ¡, Brasil\"}', name='search_internet'), type='function')])\n",
      "A previsÃ£o do tempo em Curitiba, ParanÃ¡, Brasil Ã© de temperaturas entre 18Â°C e 25Â°C, com possibilidade de chuva. A temperatura mÃ¡xima Ã© de 25Â°C. Ã‰ recomendÃ¡vel conferir sites de meteorologia como Climatempo ou AccuWeather para obter informaÃ§Ãµes atualizadas sobre a previsÃ£o do tempo.\n"
     ]
    }
   ],
   "source": [
    "# Groq Tool Calling\n",
    "# Example using duckduckgo_search\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "# Groq Tool Calling\n",
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize the Groq client\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# Specify the model to be used \n",
    "MODEL = 'llama-3.3-70b-versatile'\n",
    "\n",
    "def search_internet(question):\n",
    "    \"\"\"\n",
    "    Searches the internet for the given question.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform the search using DDGS\n",
    "        results = DDGS().text(question, max_results=5)\n",
    "        return json.dumps(results)\n",
    "    except Exception as e:\n",
    "        print(f'Error occurred during search: {e}')\n",
    "        return json.dumps({\"error\": \"Failed to perform search\"})\n",
    "\n",
    "# imports calculate function from step 1\n",
    "def run_conversation(user_prompt):\n",
    "    # Initialize the conversation with system and user messages\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a tool use assistant. \\\n",
    "                Always use the search_internet function to answer questions or retrieve information from the web. \\\n",
    "                Do not try to answer without using this tool. \\\n",
    "                Answer in Portuguese\"\n",
    "\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "    # Define the available tools (i.e. functions) for our model to use\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_internet\",\n",
    "                \"description\": \"Get the latest information from the internet by searching\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"question\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The question you want to search on the internet\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"question\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    # Make the initial API call to Groq\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, # LLM to use\n",
    "        messages=messages, # Conversation history\n",
    "        stream=False,\n",
    "        tools=tools, # Available tools (i.e. functions) for our LLM to use\n",
    "        tool_choice=\"auto\", # Let our LLM decide when to use tools\n",
    "        max_tokens=4096 # Maximum number of tokens to allow in our response\n",
    "    )\n",
    "    # Extract the response and any tool call responses\n",
    "    response_message = response.choices[0].message\n",
    "    print('## Response Message:', response_message)\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        # Define the available tools that can be called by the LLM\n",
    "        available_functions = {\n",
    "            \"search_internet\": search_internet,\n",
    "        }\n",
    "        # Add the LLM's response to the conversation\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Process each tool call\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            # Call the tool and get the response\n",
    "            function_response = function_to_call(\n",
    "                question=function_args.get(\"question\")\n",
    "            )\n",
    "            # Add the tool response to the conversation\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"role\": \"tool\", # Indicates this message is from tool use\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        # Make a second API call with the updated conversation\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Return the final response\n",
    "        return second_response.choices[0].message.content\n",
    "    \n",
    "# Example usage\n",
    "user_prompt = \"What is the weather prevision in Curitiba, ParanÃ¡ in Brazil?\"\n",
    "print(run_conversation(user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parallel Tool Use**\n",
    "\n",
    "Workflow where multiple tools can be called simultaneously, enabling more efficient and effective responses.\n",
    "\n",
    "> _Note: Parallel tool use is natively enabled for all Llama 3 and Llama 3.1 models!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in New York is sunny and the weather in London is rainy.\n"
     ]
    }
   ],
   "source": [
    "# Parallel Tool Example - tool for getting temperature and a tool for getting weather condition\n",
    "# Define weather tools\n",
    "def get_temperature(location: str):\n",
    "    # This is a mock tool/function. In a real scenario, you would call a weather API.\n",
    "    temperatures = {\"New York\": 22, \"London\": 18, \"Tokyo\": 26, \"Sydney\": 20}\n",
    "    return temperatures.get(location, \"Temperature data not available\")\n",
    "\n",
    "def get_weather_condition(location: str):\n",
    "    # This is a mock tool/function. In a real scenario, you would call a weather API.\n",
    "    conditions = {\"New York\": \"Sunny\", \"London\": \"Rainy\", \"Tokyo\": \"Cloudy\", \"Sydney\": \"Clear\"}\n",
    "    return conditions.get(location, \"Weather condition data not available\")\n",
    "\n",
    "# Define system messages and tools\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in New York and London?\"},\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature\",\n",
    "            \"description\": \"Get the temperature for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_condition\",\n",
    "            \"description\": \"Get the weather condition for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Make the initial request\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "# Process tool calls\n",
    "messages.append(response_message)\n",
    "\n",
    "available_functions = {\n",
    "    \"get_temperature\": get_temperature,\n",
    "    \"get_weather_condition\": get_weather_condition,\n",
    "}\n",
    "\n",
    "for tool_call in tool_calls:\n",
    "    function_name = tool_call.function.name\n",
    "    function_to_call = available_functions[function_name]\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"tool\",\n",
    "            \"content\": str(function_response),\n",
    "            \"tool_call_id\": tool_call.id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Make the final request with tool call results\n",
    "final_response = client.chat.completions.create(\n",
    "    model=MODEL, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")\n",
    "\n",
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Tool Use with Structured Outputs**\n",
    "\n",
    "Benefits of Using Structured Outputs\n",
    "\n",
    "- Type Safety: Pydantic models ensure that output adheres to the expected structure, reducing the risk of errors.\n",
    "- Automatic Validation: Instructor automatically validates the model's output against the defined schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What's the weather like in San Francisco?\n",
      "Tool: get_weather_info\n",
      "Parameters: {\"location\": \"San Francisco\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the tool schema\n",
    "tool_schema = {\n",
    "    \"name\": \"get_weather_info\",\n",
    "    \"description\": \"Get the weather information for any location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The location for which we want to get the weather information (e.g., New York)\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define the Pydantic model for the tool call\n",
    "class ToolCall(BaseModel):\n",
    "    input_text: str = Field(description=\"The user's input text\")\n",
    "    tool_name: str = Field(description=\"The name of the tool to call\")\n",
    "    tool_parameters: str = Field(description=\"JSON string of tool parameters\")\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    tool_calls: list[ToolCall]\n",
    "\n",
    "# Patch Groq() with instructor\n",
    "client = instructor.from_groq(Groq(), mode=instructor.Mode.JSON)\n",
    "\n",
    "def run_conversation(user_prompt):\n",
    "    # Prepare the messages\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are an assistant that can use tools. You have access to the following tool: {tool_schema}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Make the Groq API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        response_model=ResponseModel,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "    return response.tool_calls\n",
    "\n",
    "# Example usage\n",
    "user_prompt = \"What's the weather like in San Francisco?\"\n",
    "tool_calls = run_conversation(user_prompt)\n",
    "\n",
    "for call in tool_calls:\n",
    "    print(f\"Input: {call.input_text}\")\n",
    "    print(f\"Tool: {call.tool_name}\")\n",
    "    print(f\"Parameters: {call.tool_parameters}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the code is not to immediately answer the user's prompt but rather to identify that a tool (in this case, get_weather_info) needs to be invoked to retrieve the required information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "updated-langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
