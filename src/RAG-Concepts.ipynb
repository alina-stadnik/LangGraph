{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retrieval Augmented Generation (RAG)**\n",
    "\n",
    "- Ingestion Phase: Ingest data to the vector store (load, split, embed, store).\n",
    "- RAG Phase: Question ➜ Retrieve (similarity search against the vector store) ➜ Prompt (pass the retrieved documents and the question to the LLM) ➜ LLM ➜ Answer\n",
    "\n",
    "#### **Relevant topics about RAG:**\n",
    "\n",
    "- **Raw Data**: It's foundational content that is in the vector store for late use. It can include a wide range of data sources (csv files, pdf, websites, db, etc). **Essential part of a RAG chatbot** ➜ data trash = RAG app trash.\n",
    "- **Chunking method**: This is part of the ingestion phase. Chunking involves breaking down large documents into smaller manageable pieces that is important for ensuring the model can process and retrieve relevant information efficiently. Modern frameworks offer text chunkers, such as text splitter, but they struggle to create chunks that capture semantic meaning (too much content in chunk or not enough content).\n",
    "- **Embedding models**: It converts text into vectors representing the semantic meaning of the content. The quality of the embeddings directly impacts how well the system can understand and retrieve relevant information. Popular similarity metric: cosine similarity.\n",
    "- **Vectorstores** (triggerwarning): It stores embeddings AND raw documents. Retrieve documents via similarity search. There are different vectores, but in general embeddings don't scale well. Latency LLM > Latency VectorStore.\n",
    "- **Chatmodel**: It's the core engine that generates responses in a conversation. \"Brain\" of the RAG application, because it's not only understand the query but also integrate retrieved information to deliver coherent and contextually appropriate replies. Performance is critical for classification, Tool-Usage, Answer-Generation (withh retrieved docs) and more.\n",
    "- **LLM Based Routing**: RAG is often not the way to go. Sometimes RAG does not perform well if data is not suited for RAG, such as tabular data. Routing involver directing user queries to a more suitable system. One approach is that you let an LLM write SQL and then use the output of the LLM to query the database, which might work better with tabular data.\n",
    "- **Tool Calling**: It integrates external tools or APIIs to extend chatbots knowledge beyond RAG. Allows LLM to 'talk' to the ourside world. Example usacase: get real time information.\n",
    "- **Multiquery Retrieval**: Query the VectorStore with all 5 (or 6) questions and get more documents and (hopefully) better answers. It's normal to get the same 5 documents.\n",
    "- **Prompt Engineering**: It is the process of designing and refining input prompts to guide AI models like GPT4 in generatin accurate, relevant, and contextually appropriate responses. An effective prompt enginerring can significantly enhance the quality of responses by providing clear and precise instructions to the model.\n",
    "- **Rephrasing**: Reformulate last question based on the history of a conversation. It's important for RAG.\n",
    "- **Reranking**: Reorder/Rerank documents retrieved from Vectorstore. Reranking normally is done via Cross-Encoder model, which is a type of a model that given a query and a dcoument pair will output a similarity score to display how well a document is suited to answer a question. The score of similarity in embedding model compress the complete meaning of text into a vector space which means it loses a lot of information. Rerankers pass one document and one query to the model and get a score and the results are more accurate. \n",
    "- **LLM based document compression**: You ask to the LLM to evaluate a document and the question if it's sued to answer the question or not. The model should return a 'True' or 'False'. If it's false, then drop the documents from the documents list.\n",
    "- **Agentic RAG**: An agent is cyclic so it can run in a loop. Agents allow for techniques like self-refection or hierarchical teams. It can be expensive and slow.\n",
    "- **Raptor** (Recursive abstractive processing for tree-organized retrieval): Raptor is a technique designed to preserve overarching content in a text by clustering and embedding information across multiple levels. Goal: context-aware information retrieval across large texts (usually text splitter struggles with this). It is compuntationally expensive and hard to understand. \n",
    "- **Graph-RAG**: Makes use of knowledge-graphs. It can be complex as Raptor.\n",
    "- **Guardrails**: Prevent LLM from talking about specific topics and more. It's crucial while using LLM App in production.\n",
    "- **Model finetuning**: Customizing model to perform better in specific tasks. Fine-tuning is the process of taking a pre-trained model and continuing its training on a new, specific dataset by just the final layers. *Use finetuning for*: set specific style, tone, format; improve realiability to create specific output (JSON-mode); handle edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAG-Tierlist**\n",
    "\n",
    "**S-Tier**\n",
    "- **Raw Data**\n",
    "- **Chat Model**\n",
    "\n",
    "**A-Tier**\n",
    "- **Document Chunking**\n",
    "- **Prompt Engineering**\n",
    "- **Guardrails**\n",
    "\n",
    "**B-Tier**\n",
    "- **Embedding Models**\n",
    "- **Tool Calling**\n",
    "- **Rephrasing**\n",
    "- **LLM-Based Document Compression**\n",
    "- **Agentic RAG**\n",
    "\n",
    "**C-Tier**\n",
    "- **VectorStore**\n",
    "- **Reranking**\n",
    "\n",
    "**D-Tier**\n",
    "- **LLM-Based Routing**\n",
    "- **Multiquery Retrieval**\n",
    "- **Raptor**\n",
    "- **Graph-RAG**\n",
    "- **Model Fine-Tuning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "support-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
