{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAG with Groq and Llama 3**\n",
    "\n",
    "This notebook demonstrates a complete workflow for building a semantic search and retrieval-based question-answering system using ChromaDB as a persistent vector store. The key components include dataset preparation, embedding generation, vector storage, document retrieval, and answer generation.\n",
    "\n",
    "**Workflow**:\n",
    "\n",
    "1. **Dataset Preparation**:\n",
    "    - A dataset is loaded using the `datasets` library.\n",
    "    - Unnecessary columns are removed to streamline data processing.\n",
    "2. **Embedding Generation**:\n",
    "    - Utilizes the `e5-base-4k` model from the HuggingFace `semantic_router.encoders` module for encoding the dataset into vector embeddings.\n",
    "3. **Vector Storage**:\n",
    "    - Embeddings are stored in a ChromaDB persistent vector database.\n",
    "    - Data is added to the vector store in batches for efficiency.\n",
    "4. **Document Retrieval**:\n",
    "    - Implements a query function to retrieve relevant documents based on semantic similarity from ChromaDB.\n",
    "    - Tests retrieval accuracy by verifying the results against input queries.\n",
    "5. **Answer Generation**:\n",
    "    - Generates answers to user queries using the Groq API, leveraging retrieved documents for contextual grounding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation**:\n",
    "\n",
    "We start by downloading a dataset that we will encode and store. The dataset `jamescalam/ai-arxiv2-semantic-chunks` contains scraped data from many popular ArXiv papers centred around LLMs and GenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using with HuggingFaceEncoder\n",
    "#!pip install -qU \"semantic-router[local]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Versions:\n",
      "datasets: 3.1.0\n",
      "groq: 0.13.0\n",
      "semantic_router: 0.0.72\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import groq\n",
    "import semantic_router\n",
    "\n",
    "print(\"Library Versions:\")\n",
    "print(f\"datasets: {datasets.__version__}\")\n",
    "print(f\"groq: {getattr(groq, '__version__', 'Version attribute not found')}\")\n",
    "print(f\"semantic_router: {getattr(semantic_router, '__version__', 'Version attribute not found')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'content', 'prechunk_id', 'postchunk_id', 'arxiv_id', 'references'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"jamescalam/ai-arxiv2-semantic-chunks\",\n",
    "    split=\"train[:10000]\"\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 200K chunks, where each chunk is roughly the length of 1-2 paragraphs in length. Here is an example of a single record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2401.04088#0',\n",
       " 'title': 'Mixtral of Experts',\n",
       " 'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.',\n",
       " 'prechunk_id': '',\n",
       " 'postchunk_id': '2401.04088#1',\n",
       " 'arxiv_id': '2401.04088',\n",
       " 'references': ['1905.07830']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the data into the format we need, this will contain `id`, `text` (which we will embed), and `metadata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'metadata'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda x: {\n",
    "    \"id\": x[\"id\"],\n",
    "    \"metadata\": {\n",
    "        \"title\": x[\"title\"],\n",
    "        \"content\": x[\"content\"],\n",
    "    }\n",
    "})\n",
    "\n",
    "# drop uneeded columns\n",
    "data = data.remove_columns([\n",
    "    \"title\", \"content\", \"prechunk_id\",\n",
    "    \"postchunk_id\", \"arxiv_id\", \"references\"\n",
    "])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2401.04088#0',\n",
       " 'metadata': {'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.',\n",
       "  'title': 'Mixtral of Experts'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define an embedding model to create our embedding vectors for retrieval, for that we will be using a variation of the e5-base model with a longer context length of 4k tokens. Ideally we should be running this on GPU for optimal runtimes.\n",
    "\n",
    "> 'multilingual-e5-base' model has support for over 100 languages, including Portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "\n",
    "encoder = HuggingFaceEncoder(name=\"dwzhu/e5-base-4k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Config', '__abstractmethods__', '__annotations__', '__call__', '__class__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_decompose_class', '_enforce_dict_if_root', '_get_value', '_init_private_attributes', '_initialize_hf_model', '_iter', '_max_pooling', '_mean_pooling', '_model', '_tokenizer', '_torch', 'acall', 'construct', 'copy', 'device', 'dict', 'from_orm', 'json', 'model_kwargs', 'name', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'score_threshold', 'set_score_threshold', 'tokenizer_kwargs', 'type', 'update_forward_refs', 'validate']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the available methods\n",
    "print(dir(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(4096, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder._model)  # just checking the _model attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether our encoder will use cpu or a cuda GPU (where available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create embeddings now like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = encoder([\"this is a test\"]) # the class implements a __call___ method, makint it directly callable to generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the dimensionality of our returned embeddings, which we'll need soon when initializing our vector index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = len(embeds[0]) \n",
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import PersistentClient, EmbeddingFunction \n",
    "\n",
    "# Define the directory for persistent storage\n",
    "persist_dir = \"../chromadb_persist\"\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "db = PersistentClient(path=persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEncoder(name='dwzhu/e5-base-4k', score_threshold=0.5, type='huggingface', tokenizer_kwargs={}, model_kwargs={}, device='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the collection already exists\n",
    "existing_collections = [collection.name for collection in db.list_collections()]\n",
    "\n",
    "# Delete the collection\n",
    "#db.delete_collection(name=existing_collections[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4229b081b407400bbadac285e99ae88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Create a collection in ChromaDB\n",
    "collection_name = \"groq_llama_3_rag\"\n",
    "collection = db.get_or_create_collection(\n",
    "    name=collection_name, \n",
    "    )\n",
    "\n",
    "# Insert embeddings into ChromaDB\n",
    "batch_size = 128\n",
    "\n",
    "# Check if the collection already exists\n",
    "existing_collections = [collection.name for collection in db.list_collections()]\n",
    "\n",
    "if collection_name not in existing_collections:\n",
    "    # Create a new collection if it doesn't exist\n",
    "    collection = db.create_collection(\n",
    "        name=collection_name)\n",
    "    \n",
    "else:\n",
    "    # Retrieve the existing collection\n",
    "    collection = db.get_collection(name=collection_name)\n",
    "\n",
    "# Start populating the collection with embeddings\n",
    "batch_size = 128  # How many embeddings to insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    # Find end of batch\n",
    "    i_end = min(len(data), i + batch_size)\n",
    "    # Create batch\n",
    "    batch = data[i:i_end]\n",
    "\n",
    "    # Extract metadata (ensure correct length)\n",
    "    batch_metadata = batch['metadata']  # This should have the same number of entries as the batch size\n",
    "\n",
    "    # Use the 'id' from the data for the current batch\n",
    "    ids = batch['id']  # Assuming 'id' is a field in your data that is already unique and correct\n",
    "\n",
    "    # Check that batch size matches metadata and IDs length\n",
    "    assert len(batch_metadata) == len(ids), f\"Batch size mismatch: {len(batch_metadata)} metadata vs {len(ids)} IDs\"\n",
    " \n",
    "    # Generate embeddings from content\n",
    "    chunks = [f'{x[\"title\"]}: {x[\"content\"]}' for x in batch_metadata]\n",
    "    embeds = encoder(chunks)  # Directly using encoder\n",
    "    \n",
    "    # Check if embedding length matches the expected size\n",
    "    assert len(embeds) == len(ids), f\"Mismatch between number of embeddings ({len(embeds)}) and IDs ({len(ids)})\"\n",
    "\n",
    "    # Prepare data for insertion into ChromaDB\n",
    "    to_upsert = list(zip(ids, embeds, batch_metadata))\n",
    "    \n",
    "    # Insert embeddings into ChromaDB\n",
    "    collection.add(embeddings=embeds, metadatas=batch_metadata, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '4 . So the answer is 9Ï 4 . Are there variables in the solution? the form of \"1. variable is defined as...\". If so, please list the definition of variable in The underlined parts are the type of question, the question itself and the steps in its solution, respectively. The output from the LLM is: Yes. There are variables in the solution. x + yi, where xxx and yyy are real numbers. x + yi 1. zzz is defined as a complex number of the form x + yi',\n",
       "  'title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning'},\n",
       " {'content': 'The bold part is then saved to form a part of the input in the regeneration stage. Target extraction To get a brief and clear target of the current step, the input to the LLM is: The following is a part of the solution to the problem: Let S be the set of complex numbers z such that the real part of 1 6 . This set forms a curve. Find the area of the 12 region inside the curve. (Step 0) Let z = x + yi be a complex number, where x and y are real numbers. (Step 5) Completing the square, we obtain (a â 3)? +y= Q. 3)? +y= Q. 2 . . . : 2 What specific action does the step \"Completing the square, we obtain (a â 3) take? Please give a brief answer using a single sentence and do not copy the steps. 2 2 +y= 4 .\" The underlined parts are the question and reasoning steps before the current one, including the current one. The output of the LLM is: The step completes the square to rewrite the equation in standard form of a circle. The whole sentence is saved and forms the most important part of the input in the regeneration stage. Information Collection To get sentences in the question and previous steps in the solution that are directly related to the current step, the input to the LLM is: This is a math question: Question: Let S be the set of complex numbers z such that the real part of 1 6 . This set forms a curve. Find the area of the region inside the curve.',\n",
       "  'title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning'},\n",
       " {'content': 'The following is information extracted from the question: Information 0: Let S be the set of complex numbers z such that the real part of 1 1 6 . Information 1: This set forms a curve. Information 2: Find the area of the region inside the curve. The following is the first a few steps in a solution to the problem: Step 0: Let z = x + yi be a complex number, where x and y are real numbers. ... Step 4: Cross-multiplying and rearranging, we get the equation 6x = x2 + y2. Which previous steps or information does the next step \"Completing the square, we obtain (x â 3) _ p= on\" directly follow from? _ p= 4 .\" directly follow from? 2 The underlined information (contents after Information X:) is simply a list of sentences in the question. The underlined steps are reasoning steps before (not including) the current one in the solution. The output from the LLM is: The next step \"Completing the square, we obtain (x â 3)? +y? = 2.\" directly follows from Step 4. Then we use regular expression to extract the information and step id from LLMâ s output. Some steps may only rely on information from the question while others may only use results from previous steps. Step regeneration In this stage, we instruct the LLM to achieve the extracted target with only collected information with the following prompt: We are in a process of solving a math problem.',\n",
       "  'title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning'},\n",
       " {'content': 'Variables are defined as: 1. z is defined as a complex number of the form x + yi, where x and y are real numbers. The following are some previous steps: Step 0: Cross-multiplying and rearranging, we get the equation 6x = x2 + y2. 13 The target for next step is: The step completes the square to rewrite the equation in standard form of a circle. Please try to achieve the target with the information from previous steps. The underlined parts are the type of question, variable definition (if any), collected information from the question (if any), collected previous steps (if any) and the extracted target. Please notice that the current step does not directly rely on any information from the question, we removed the corresponding block from the input. The output from the LLM is:',\n",
       "  'title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning'},\n",
       " {'content': 'To rewrite the equation in standard form of a circle, we need to complete the square for both xxx and yyy. 6x = x2 + y2 Starting from the given equation: 6x = x2 + y2 6x = x2 + y2. (6/2)2 = 9 We add and subtract (6/2)2 = 9 (6/2)2 = 9 on the right-hand side: 6x + 9 â 9 = x2 + y2 + 9 â 9 6x + 9 â 9 = x2 + y2 + 9 â 9. 6x + 9 â 9 = x2 + y2 + 9 â 9 (x â 3)2 + y2 = 9 Simplifying: (x â 3)2 + y2 = 9 (x â 3)2 + y2 = 9. This is now in the standard form of a circle with (3, 0) and radius 333. (3, 0) center (3, 0)',\n",
       "  'title': 'SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['metadata'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(query: str, top_k: int, collection) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents from ChromaDB based on a query.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The input query string.\n",
    "        top_k (int): The number of top results to retrieve.\n",
    "        collection: The ChromaDB collection to query.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of document content strings.\n",
    "    \"\"\"\n",
    "    # Encode the query into an embedding\n",
    "    xq = encoder([query])\n",
    "    \n",
    "    # Query the ChromaDB collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=xq,\n",
    "        n_results=top_k,\n",
    "        include=[\"metadatas\", \"distances\"]  \n",
    "    )\n",
    "\n",
    "    #print(results[\"distances\"])  # Check similarity scores\n",
    "    #print(results[\"metadatas\"]) # it's a nested list of dicts\n",
    "    \n",
    "    # Extract document content from nested metadata structure\n",
    "    docs = [metadata.get(\"content\", \"\") for sublist in results[\"metadatas\"] for metadata in sublist]\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. CoRR, abs/2206.05802, 2022. doi: 10.48550/arXiv.2206.05802. URL https://doi.org/10.48550/arXiv.2206.05802.', ') between the evaluation scores of various LLM evaluators and those of human-annotated scores, on MM-ReAct-GPT4â s results. A smaller discrepancy indicates a better agreement with the gold standard of human evaluation, indicating a better evaluator. Model â (â ) Keyword matching 0.273 LLM-based evaluation LLaMA-2-7B LLaMA-2-13B GPT-3.5 (turbo-0613) GPT-4 (0613) 0.307 0.254 0.178 0.042 â ¢ Analysis of open-source LMMs (Section 4.3.1) leaves room for ambiguity regarding the superior vision encoders for LMMs, based on current model comparisons. However, it is evident that stronger LLMs can boost the performance of LMMs.', 'â ¢ LLaMA-2-7B (Touvron et al., 2023b): it is an updated version of LLaMA (Touvron et al., 2023a). It has been pre-trained on a mixture of publicly available online data of 2T tokens. # 2.2 Results and Analysis We report the evaluation results of LLMs after train- ing with the benchmark leakage settings in Table 1 and Table 2. Overall, different levels of data leak- age result in inflated model performance on bench- marks. We have the following observations. evaluation into an in-domain test task, making it easier for LLMs to achieve higher results. An in- triguing finding occurs when we examine the result on the Chinese benchmark C3-Dialog. Despite the pre-training corpus of the four LLMs containing very little Chinese data, using training sets doubles their evaluation scores, e.g., elevating GPT-Neo- 1.3Bâ s score from 24.18 to 48.62. This observation underscores the significance of avoiding training set leakage in pre-training, as it can lead to spuri- ous performance improvements that distort the real assessment of model capabilities. First, we can see that using MMLU training set can greatly boost the evaluation results on the MMLU benchmark. However, this improvement comes at the cost of decreased performance on tasks unrelated to MMLU, (such as HellaSwag and GSM8k about commonsense and mathemati- cal knowledge, respectively), suggesting that over- emphasizing a specific task may lower the model generalization capability. Besides, when incorpo- rating all the training sets of the evaluated bench- marks, there is a notable performance increase across almost all the evaluated tasks. Incorporating training data converts the original zero/few-shot Second, the evaluation scores continue to rise as the data leakage becomes more severe. Remark- ably, when the test prompts were leaked, smaller LLMs can even surpass much larger LLMs that were not trained with leaked data, e.g., â phi-1.5- 1.3B+All Train S+Test Pâ outperforms LLaMA- 65B on RACE-M (55.80 vs. 53.00) and RACE-H (52.82 vs. 48.00).', '# A LLM Prompts In this section, we provide the prompts used in the CALYPSO system. Generally, we make use of Markdown-style headers to divide sections of the prompt. For chat-based models, we annotate each message with the corresponding role (system, assistant, or user, as exposed in the ChatGPT API). # A.1 Encounter Understanding Summarization Summarize the following D&D setting and monsters for a Dungeon Masterâ s notes without mentioning game stats. Setting ======= <Setting description inserted here.> Creatures ========= <Name> ------ <Statistics and lore inserted here. If the encounter involves multiple creatures, repeat for each creature.> Summary ======= Abstractive Understanding Your name is Calypso, and your job is to help the Dungeon Master with an encounter. Your task is to help the DM understand the setting and creatures as a group, focusing mainly on appearance and how they act. Especially focus on what makes each creature # stand out. Avoid mentioning game stats.', 'Multimodal Bard: There are 8 models in the figure that achieve an Acc score greater than 60. These models are: GPT-4 * GPT3-175B * LLAMA-7B * LLAMA2-7B * LLAMA-13B * LLAMA2-13B * LLAMA-33B * LLAMA-65B * LLAMA2-70B The Acc score is the accuracy of the model, which is the percentage of test cases that the model correctly classifies. An Acc score of 60 means that the model correctly classifies 60% of the test cases.']\n"
     ]
    }
   ],
   "source": [
    "collection = db.get_collection(\"groq_llama_3_rag\")\n",
    "\n",
    "# Query ChromaDB\n",
    "query = \"can you tell me about the Llama LLMs?\"\n",
    "top_k = 5\n",
    "documents = get_docs(query, top_k, collection)\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our retrieval component works, now let's try feeding this into a Llama 3 70B model hosted by Groq to produce an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize the Groq client\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_client = Groq(api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query: str, docs: list[str]):\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant that answers questions about AI using the \"\n",
    "        \"context provided below.\\n\\n\"\n",
    "        \"CONTEXT:\\n\"\n",
    "        \"\\n---\\n\".join(docs)\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    # generate response\n",
    "    chat_response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context provided, LLaMA refers to a family of Large Language Models (LLMs) developed by Meta AI. There are several variants of LLaMA mentioned, including:\n",
      "\n",
      "1. LLaMA-2-7B: An updated version of LLaMA, pre-trained on a mixture of publicly available online data of 2 trillion tokens.\n",
      "2. LLaMA-2-13B: A larger version of LLaMA-2-7B, with a larger model size.\n",
      "3. LLaMA-13B: Another variant of LLaMA with a larger model size.\n",
      "4. LLaMA-33B: A larger version of LLaMA with an even larger model size.\n",
      "5. LLaMA-65B: The largest variant of LLaMA mentioned, with a model size of 65 billion parameters.\n",
      "6. LLaMA-2-70B: Another variant of LLaMA with a model size of 70 billion parameters.\n",
      "\n",
      "These LLaMA models are compared to other LLMs, such as GPT-3.5 and GPT-4, in terms of their performance on various benchmarks and tasks. The results suggest that stronger LLMs, like LLaMA-65B, can boost the performance of Multimodal Models (MMMs) and achieve better agreements with human evaluation scores.\n"
     ]
    }
   ],
   "source": [
    "out = generate(query=query, docs=documents)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, here's what I can tell you about LLM (Large Language Models) and finetuning:\n",
      "\n",
      "**LLMs:**\n",
      "\n",
      "* LLMs are large-scale language models that have been pre-trained on vast amounts of text data.\n",
      "* They can be fine-tuned for specific tasks, such as evaluation, summarization, and understanding.\n",
      "* Examples of LLMs mentioned in the context include GPT-3.5, GPT-4, LLaMA-2-7B, LLaMA-2-13B, and others.\n",
      "\n",
      "**Finetuning:**\n",
      "\n",
      "* Finetuning refers to the process of adapting a pre-trained LLM to a specific task or dataset.\n",
      "* During finetuning, the model is trained on a smaller dataset specific to the task, which adjusts the model's weights to better fit the task at hand.\n",
      "* Finetuning can significantly improve the performance of LLMs on specific tasks, as seen in the context where using training sets can greatly boost the evaluation results on specific benchmarks.\n",
      "* However, over-emphasizing a specific task through finetuning can lead to decreased performance on other tasks, as observed in the context.\n",
      "\n",
      "In the context, it seems that the authors are exploring the effects of finetuning on LLMs, studying how different levels of data leakage can impact the performance of these models on various tasks. They also highlight the importance of avoiding training set leakage in pre-training to ensure a more accurate assessment of model capabilities.\n",
      "\n",
      "Do you have any specific questions about LLMs or finetuning that I can help with?\n"
     ]
    }
   ],
   "source": [
    "another_query='Tell me about LLM and finetuning'\n",
    "out = generate(query=another_query, docs=documents)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "updated-langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
